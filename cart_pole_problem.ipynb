{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement: \n",
    "Cartpole - known also as an Inverted Pendulum is a pendulum with a center of gravity above its pivot point. Itâ€™s unstable, \n",
    "but can be controlled by moving the pivot point under the center of mass. The goal is to keep the cartpole balanced by \n",
    "applying appropriate forces to a pivot point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\dell\\anacondanew\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\anacondanew\\lib\\site-packages (from gym) (1.2.1)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from gym) (1.4.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from gym) (1.16.2)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\anacondanew\\lib\\site-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from gym) (1.2.1)\n",
      "Requirement already satisfied: future in c:\\users\\dell\\anacondanew\\lib\\site-packages (from pyglet>=1.2.0->gym) (0.17.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\dell\\anacondanew\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from h5py) (1.16.2)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\anacondanew\\lib\\site-packages (from h5py) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\dell\\anacondanew\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras) (1.16.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras) (1.2.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras) (5.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\dell\\anacondanew\\lib\\site-packages (1.14.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorflow) (0.33.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorflow) (0.1.7)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorflow) (1.16.2)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorflow) (1.11.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorflow) (1.22.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.14.1)\n",
      "Collecting setuptools>=41.0.0 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/ec/51/f45cea425fd5cb0b0380f5b0f048ebc1da5b417e48d304838c02d6288a1e/setuptools-41.0.1-py2.py3-none-any.whl (575kB)\n",
      "Requirement already satisfied: h5py in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
      "Installing collected packages: setuptools\n",
      "  Found existing installation: setuptools 40.8.0\n",
      "    Uninstalling setuptools-40.8.0:\n",
      "      Successfully uninstalled setuptools-40.8.0\n",
      "Successfully installed setuptools-41.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-rl in c:\\users\\dell\\anacondanew\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: keras>=2.0.7 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras-rl) (2.2.4)\n",
      "Requirement already satisfied: h5py in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras>=2.0.7->keras-rl) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras>=2.0.7->keras-rl) (1.16.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras>=2.0.7->keras-rl) (5.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras>=2.0.7->keras-rl) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras>=2.0.7->keras-rl) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\dell\\anacondanew\\lib\\site-packages (from keras>=2.0.7->keras-rl) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-rl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0806 19:13:54.943631  5012 deprecation_wrapper.py:119] From C:\\Users\\Dell\\Anacondanew\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0806 19:13:56.061696  5012 deprecation_wrapper.py:119] From C:\\Users\\Dell\\Anacondanew\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0806 19:13:56.687731  5012 deprecation_wrapper.py:119] From C:\\Users\\Dell\\Anacondanew\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0806 19:13:56.952746  5012 deprecation_wrapper.py:119] From C:\\Users\\Dell\\Anacondanew\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0806 19:13:56.954746  5012 deprecation_wrapper.py:119] From C:\\Users\\Dell\\Anacondanew\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0806 19:13:58.911859  5012 deprecation_wrapper.py:119] From C:\\Users\\Dell\\Anacondanew\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anacondanew\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   79/5000: episode: 1, duration: 6.819s, episode steps: 79, steps per second: 12, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.060 [-0.402, 0.722], loss: 0.428072, mean_absolute_error: 0.495901, mean_q: 0.052834\n",
      "  113/5000: episode: 2, duration: 0.155s, episode steps: 34, steps per second: 219, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.151 [-0.159, 0.753], loss: 0.351183, mean_absolute_error: 0.445727, mean_q: 0.190834\n",
      "  163/5000: episode: 3, duration: 0.250s, episode steps: 50, steps per second: 200, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.082 [-0.295, 0.778], loss: 0.313844, mean_absolute_error: 0.465298, mean_q: 0.319593\n",
      "  197/5000: episode: 4, duration: 0.160s, episode steps: 34, steps per second: 212, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.081 [-0.228, 0.770], loss: 0.274037, mean_absolute_error: 0.498487, mean_q: 0.468174\n",
      "  261/5000: episode: 5, duration: 0.335s, episode steps: 64, steps per second: 191, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.394, 0.861], loss: 0.231166, mean_absolute_error: 0.562121, mean_q: 0.681503\n",
      "  295/5000: episode: 6, duration: 0.233s, episode steps: 34, steps per second: 146, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.207, 0.836], loss: 0.184100, mean_absolute_error: 0.650647, mean_q: 0.927358\n",
      "  330/5000: episode: 7, duration: 0.174s, episode steps: 35, steps per second: 202, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.076 [-0.348, 0.926], loss: 0.142522, mean_absolute_error: 0.715095, mean_q: 1.151363\n",
      "  359/5000: episode: 8, duration: 0.151s, episode steps: 29, steps per second: 192, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.086 [-0.395, 0.805], loss: 0.114063, mean_absolute_error: 0.804056, mean_q: 1.383229\n",
      "  381/5000: episode: 9, duration: 0.106s, episode steps: 22, steps per second: 208, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.353, 0.912], loss: 0.100097, mean_absolute_error: 0.871005, mean_q: 1.567104\n",
      "  401/5000: episode: 10, duration: 0.100s, episode steps: 20, steps per second: 200, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.082 [-0.413, 0.874], loss: 0.088156, mean_absolute_error: 0.938282, mean_q: 1.742352\n",
      "  423/5000: episode: 11, duration: 0.129s, episode steps: 22, steps per second: 170, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.616, 0.942], loss: 0.089835, mean_absolute_error: 1.020529, mean_q: 1.907663\n",
      "  443/5000: episode: 12, duration: 0.110s, episode steps: 20, steps per second: 182, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.087 [-0.558, 1.174], loss: 0.088773, mean_absolute_error: 1.082025, mean_q: 2.075327\n",
      "  468/5000: episode: 13, duration: 0.121s, episode steps: 25, steps per second: 207, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.080 [-0.369, 1.017], loss: 0.102324, mean_absolute_error: 1.185827, mean_q: 2.257986\n",
      "  492/5000: episode: 14, duration: 0.131s, episode steps: 24, steps per second: 183, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.078 [-0.396, 1.106], loss: 0.105469, mean_absolute_error: 1.283458, mean_q: 2.459656\n",
      "  505/5000: episode: 15, duration: 0.088s, episode steps: 13, steps per second: 147, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.094 [-0.790, 1.260], loss: 0.106334, mean_absolute_error: 1.355559, mean_q: 2.635272\n",
      "  517/5000: episode: 16, duration: 0.081s, episode steps: 12, steps per second: 148, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.114 [-0.776, 1.266], loss: 0.093625, mean_absolute_error: 1.400203, mean_q: 2.739536\n",
      "  536/5000: episode: 17, duration: 0.126s, episode steps: 19, steps per second: 150, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.071 [-0.587, 1.286], loss: 0.096792, mean_absolute_error: 1.459634, mean_q: 2.895322\n",
      "  554/5000: episode: 18, duration: 0.082s, episode steps: 18, steps per second: 218, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.079 [-0.609, 1.188], loss: 0.153633, mean_absolute_error: 1.569184, mean_q: 3.068320\n",
      "  572/5000: episode: 19, duration: 0.087s, episode steps: 18, steps per second: 208, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.100 [-0.771, 1.537], loss: 0.163632, mean_absolute_error: 1.653909, mean_q: 3.197749\n",
      "  586/5000: episode: 20, duration: 0.080s, episode steps: 14, steps per second: 176, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.110 [-0.594, 1.217], loss: 0.206902, mean_absolute_error: 1.738222, mean_q: 3.376930\n",
      "  598/5000: episode: 21, duration: 0.061s, episode steps: 12, steps per second: 198, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.137 [-0.578, 1.232], loss: 0.219033, mean_absolute_error: 1.806248, mean_q: 3.493924\n",
      "  611/5000: episode: 22, duration: 0.072s, episode steps: 13, steps per second: 180, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.111 [-0.992, 1.604], loss: 0.184376, mean_absolute_error: 1.860561, mean_q: 3.624403\n",
      "  622/5000: episode: 23, duration: 0.053s, episode steps: 11, steps per second: 208, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.127 [-0.755, 1.455], loss: 0.173013, mean_absolute_error: 1.888976, mean_q: 3.740178\n",
      "  633/5000: episode: 24, duration: 0.079s, episode steps: 11, steps per second: 140, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.121 [-0.775, 1.376], loss: 0.310949, mean_absolute_error: 1.969187, mean_q: 3.836984\n",
      "  645/5000: episode: 25, duration: 0.061s, episode steps: 12, steps per second: 197, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.092 [-1.187, 1.773], loss: 0.376279, mean_absolute_error: 2.013810, mean_q: 3.930443\n",
      "  654/5000: episode: 26, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.129 [-0.969, 1.736], loss: 0.369204, mean_absolute_error: 2.120782, mean_q: 4.059889\n",
      "  666/5000: episode: 27, duration: 0.059s, episode steps: 12, steps per second: 202, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.089 [-1.377, 2.045], loss: 0.283463, mean_absolute_error: 2.100457, mean_q: 4.130817\n",
      "  678/5000: episode: 28, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.099 [-1.225, 1.965], loss: 0.413491, mean_absolute_error: 2.197798, mean_q: 4.279879\n",
      "  689/5000: episode: 29, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.144 [-1.126, 1.784], loss: 0.363931, mean_absolute_error: 2.240257, mean_q: 4.370093\n",
      "  699/5000: episode: 30, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.125 [-1.129, 1.813], loss: 0.417749, mean_absolute_error: 2.299611, mean_q: 4.478065\n",
      "  710/5000: episode: 31, duration: 0.056s, episode steps: 11, steps per second: 196, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.112 [-1.403, 2.206], loss: 0.514737, mean_absolute_error: 2.380534, mean_q: 4.614346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  719/5000: episode: 32, duration: 0.053s, episode steps: 9, steps per second: 171, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.135 [-1.611, 2.534], loss: 0.615713, mean_absolute_error: 2.483820, mean_q: 4.703921\n",
      "  729/5000: episode: 33, duration: 0.076s, episode steps: 10, steps per second: 131, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.131 [-1.544, 2.503], loss: 0.436056, mean_absolute_error: 2.465793, mean_q: 4.741426\n",
      "  738/5000: episode: 34, duration: 0.061s, episode steps: 9, steps per second: 148, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.151 [-1.516, 2.495], loss: 0.367566, mean_absolute_error: 2.461628, mean_q: 4.829305\n",
      "  747/5000: episode: 35, duration: 0.057s, episode steps: 9, steps per second: 158, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.141 [-1.201, 1.986], loss: 0.571186, mean_absolute_error: 2.515881, mean_q: 4.941770\n",
      "  759/5000: episode: 36, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.096 [-1.005, 1.638], loss: 0.596685, mean_absolute_error: 2.601844, mean_q: 5.006228\n",
      "  771/5000: episode: 37, duration: 0.055s, episode steps: 12, steps per second: 217, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.114 [-1.161, 1.935], loss: 0.686997, mean_absolute_error: 2.674136, mean_q: 5.114184\n",
      "  780/5000: episode: 38, duration: 0.043s, episode steps: 9, steps per second: 212, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.140 [-1.157, 1.952], loss: 0.679536, mean_absolute_error: 2.715454, mean_q: 5.236325\n",
      "  791/5000: episode: 39, duration: 0.071s, episode steps: 11, steps per second: 154, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.122 [-1.375, 2.225], loss: 0.776034, mean_absolute_error: 2.757662, mean_q: 5.302556\n",
      "  800/5000: episode: 40, duration: 0.044s, episode steps: 9, steps per second: 205, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.133 [-1.384, 2.243], loss: 0.628666, mean_absolute_error: 2.768422, mean_q: 5.358373\n",
      "  808/5000: episode: 41, duration: 0.040s, episode steps: 8, steps per second: 199, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.157 [-1.175, 2.000], loss: 0.684763, mean_absolute_error: 2.846918, mean_q: 5.431595\n",
      "  817/5000: episode: 42, duration: 0.045s, episode steps: 9, steps per second: 202, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.113 [-1.223, 1.887], loss: 0.738762, mean_absolute_error: 2.902407, mean_q: 5.515232\n",
      "  827/5000: episode: 43, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.157 [-1.143, 2.083], loss: 0.886701, mean_absolute_error: 2.986606, mean_q: 5.617704\n",
      "  838/5000: episode: 44, duration: 0.064s, episode steps: 11, steps per second: 173, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.136 [-1.137, 1.900], loss: 0.645288, mean_absolute_error: 2.962918, mean_q: 5.674078\n",
      "  847/5000: episode: 45, duration: 0.046s, episode steps: 9, steps per second: 196, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.156 [-1.332, 2.266], loss: 0.764756, mean_absolute_error: 2.999206, mean_q: 5.796710\n",
      "  856/5000: episode: 46, duration: 0.047s, episode steps: 9, steps per second: 189, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.181 [-1.531, 2.551], loss: 0.751106, mean_absolute_error: 3.037164, mean_q: 5.881929\n",
      "  865/5000: episode: 47, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.159 [-1.525, 2.446], loss: 0.607139, mean_absolute_error: 3.036553, mean_q: 5.938126\n",
      "  874/5000: episode: 48, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.124 [-1.794, 2.794], loss: 0.949965, mean_absolute_error: 3.126876, mean_q: 6.043673\n",
      "  883/5000: episode: 49, duration: 0.045s, episode steps: 9, steps per second: 199, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.138 [-1.572, 2.456], loss: 0.755331, mean_absolute_error: 3.139820, mean_q: 6.054176\n",
      "  893/5000: episode: 50, duration: 0.051s, episode steps: 10, steps per second: 198, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.104 [-1.587, 2.451], loss: 0.846416, mean_absolute_error: 3.198928, mean_q: 6.097300\n",
      "  903/5000: episode: 51, duration: 0.052s, episode steps: 10, steps per second: 193, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.125 [-1.533, 2.509], loss: 1.171746, mean_absolute_error: 3.288537, mean_q: 6.155997\n",
      "  913/5000: episode: 52, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.119 [-1.790, 2.663], loss: 0.790028, mean_absolute_error: 3.257117, mean_q: 6.178496\n",
      "  923/5000: episode: 53, duration: 0.050s, episode steps: 10, steps per second: 201, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.994, 3.014], loss: 0.859337, mean_absolute_error: 3.263755, mean_q: 6.314944\n",
      "  932/5000: episode: 54, duration: 0.048s, episode steps: 9, steps per second: 188, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.765, 2.769], loss: 0.920386, mean_absolute_error: 3.338891, mean_q: 6.431572\n",
      "  942/5000: episode: 55, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.138 [-1.549, 2.572], loss: 1.429273, mean_absolute_error: 3.455478, mean_q: 6.486879\n",
      "  952/5000: episode: 56, duration: 0.078s, episode steps: 10, steps per second: 128, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.123 [-1.984, 3.051], loss: 1.109002, mean_absolute_error: 3.449048, mean_q: 6.355931\n",
      "  961/5000: episode: 57, duration: 0.062s, episode steps: 9, steps per second: 146, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.775, 2.809], loss: 1.125500, mean_absolute_error: 3.470198, mean_q: 6.450196\n",
      "  971/5000: episode: 58, duration: 0.070s, episode steps: 10, steps per second: 142, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.927, 3.118], loss: 1.029870, mean_absolute_error: 3.459499, mean_q: 6.555994\n",
      "  981/5000: episode: 59, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.132 [-1.608, 2.639], loss: 0.774569, mean_absolute_error: 3.456835, mean_q: 6.619682\n",
      "  991/5000: episode: 60, duration: 0.047s, episode steps: 10, steps per second: 214, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.997, 3.097], loss: 1.137445, mean_absolute_error: 3.588203, mean_q: 6.701546\n",
      " 1000/5000: episode: 61, duration: 0.043s, episode steps: 9, steps per second: 210, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.117 [-1.384, 2.189], loss: 1.045803, mean_absolute_error: 3.568861, mean_q: 6.799516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1010/5000: episode: 62, duration: 0.051s, episode steps: 10, steps per second: 197, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.120 [-1.560, 2.492], loss: 1.078485, mean_absolute_error: 3.630986, mean_q: 6.796435\n",
      " 1022/5000: episode: 63, duration: 0.066s, episode steps: 12, steps per second: 183, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.102 [-1.129, 1.692], loss: 1.222748, mean_absolute_error: 3.676298, mean_q: 6.787294\n",
      " 1035/5000: episode: 64, duration: 0.063s, episode steps: 13, steps per second: 206, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.103 [-0.935, 1.488], loss: 0.889639, mean_absolute_error: 3.653696, mean_q: 6.744975\n",
      " 1048/5000: episode: 65, duration: 0.065s, episode steps: 13, steps per second: 200, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.086 [-0.999, 1.680], loss: 0.881542, mean_absolute_error: 3.676086, mean_q: 6.931526\n",
      " 1060/5000: episode: 66, duration: 0.066s, episode steps: 12, steps per second: 181, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.110 [-0.965, 1.635], loss: 0.963202, mean_absolute_error: 3.764227, mean_q: 7.075195\n",
      " 1073/5000: episode: 67, duration: 0.067s, episode steps: 13, steps per second: 195, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.095 [-0.989, 1.683], loss: 0.665015, mean_absolute_error: 3.716395, mean_q: 7.135867\n",
      " 1083/5000: episode: 68, duration: 0.051s, episode steps: 10, steps per second: 194, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.133 [-0.962, 1.676], loss: 0.798720, mean_absolute_error: 3.790939, mean_q: 7.267316\n",
      " 1095/5000: episode: 69, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.101 [-0.966, 1.545], loss: 1.128334, mean_absolute_error: 3.884275, mean_q: 7.245625\n",
      " 1109/5000: episode: 70, duration: 0.070s, episode steps: 14, steps per second: 199, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.115 [-0.774, 1.384], loss: 1.286805, mean_absolute_error: 3.923736, mean_q: 7.160455\n",
      " 1120/5000: episode: 71, duration: 0.055s, episode steps: 11, steps per second: 200, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.111 [-0.826, 1.221], loss: 1.222362, mean_absolute_error: 3.936508, mean_q: 7.206308\n",
      " 1132/5000: episode: 72, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.117 [-0.568, 1.219], loss: 1.064445, mean_absolute_error: 3.900627, mean_q: 7.121634\n",
      " 1148/5000: episode: 73, duration: 0.085s, episode steps: 16, steps per second: 188, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.068 [-0.643, 1.206], loss: 1.251990, mean_absolute_error: 3.982886, mean_q: 7.273031\n",
      " 1158/5000: episode: 74, duration: 0.055s, episode steps: 10, steps per second: 182, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.145 [-0.946, 1.625], loss: 1.503815, mean_absolute_error: 4.042456, mean_q: 7.442389\n",
      " 1169/5000: episode: 75, duration: 0.076s, episode steps: 11, steps per second: 146, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.118 [-0.955, 1.474], loss: 1.161089, mean_absolute_error: 4.009365, mean_q: 7.285424\n",
      " 1181/5000: episode: 76, duration: 0.091s, episode steps: 12, steps per second: 131, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.132 [-0.745, 1.374], loss: 1.348403, mean_absolute_error: 4.041330, mean_q: 7.330293\n",
      " 1194/5000: episode: 77, duration: 0.079s, episode steps: 13, steps per second: 164, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.090 [-0.953, 1.458], loss: 1.059105, mean_absolute_error: 4.029816, mean_q: 7.445020\n",
      " 1204/5000: episode: 78, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.132 [-0.750, 1.461], loss: 0.661186, mean_absolute_error: 4.023517, mean_q: 7.660728\n",
      " 1217/5000: episode: 79, duration: 0.062s, episode steps: 13, steps per second: 211, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.114 [-0.757, 1.457], loss: 1.142840, mean_absolute_error: 4.137391, mean_q: 7.753588\n",
      " 1234/5000: episode: 80, duration: 0.089s, episode steps: 17, steps per second: 192, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.093 [-0.549, 1.030], loss: 0.703611, mean_absolute_error: 4.100320, mean_q: 7.757797\n",
      " 1254/5000: episode: 81, duration: 0.096s, episode steps: 20, steps per second: 208, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.094 [-0.399, 1.115], loss: 0.990237, mean_absolute_error: 4.182421, mean_q: 7.801924\n",
      " 1270/5000: episode: 82, duration: 0.086s, episode steps: 16, steps per second: 186, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.091 [-0.752, 1.492], loss: 1.352229, mean_absolute_error: 4.250431, mean_q: 7.761143\n",
      " 1283/5000: episode: 83, duration: 0.064s, episode steps: 13, steps per second: 204, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.099 [-0.813, 1.407], loss: 1.293192, mean_absolute_error: 4.238768, mean_q: 7.627975\n",
      " 1296/5000: episode: 84, duration: 0.064s, episode steps: 13, steps per second: 203, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.103 [-0.740, 1.241], loss: 1.099095, mean_absolute_error: 4.233451, mean_q: 7.747299\n",
      " 1316/5000: episode: 85, duration: 0.104s, episode steps: 20, steps per second: 192, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.412, 0.845], loss: 1.248859, mean_absolute_error: 4.309711, mean_q: 7.947662\n",
      " 1345/5000: episode: 86, duration: 0.135s, episode steps: 29, steps per second: 215, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.097 [-0.381, 0.806], loss: 0.943022, mean_absolute_error: 4.333676, mean_q: 8.109018\n",
      " 1456/5000: episode: 87, duration: 0.612s, episode steps: 111, steps per second: 181, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.009 [-0.556, 1.261], loss: 1.142482, mean_absolute_error: 4.517802, mean_q: 8.407989\n",
      " 1530/5000: episode: 88, duration: 0.360s, episode steps: 74, steps per second: 205, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.033 [-0.235, 0.795], loss: 1.098302, mean_absolute_error: 4.722325, mean_q: 8.882938\n",
      " 1570/5000: episode: 89, duration: 0.190s, episode steps: 40, steps per second: 210, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.421, 0.964], loss: 1.380203, mean_absolute_error: 4.911281, mean_q: 9.154108\n",
      " 1638/5000: episode: 90, duration: 0.346s, episode steps: 68, steps per second: 196, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.061 [-0.364, 0.783], loss: 1.212261, mean_absolute_error: 4.986376, mean_q: 9.341979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1679/5000: episode: 91, duration: 0.257s, episode steps: 41, steps per second: 159, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.113 [-1.142, 0.247], loss: 1.015555, mean_absolute_error: 5.087359, mean_q: 9.682144\n",
      " 1706/5000: episode: 92, duration: 0.130s, episode steps: 27, steps per second: 207, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.095 [-0.783, 0.191], loss: 1.009575, mean_absolute_error: 5.208023, mean_q: 9.938776\n",
      " 1730/5000: episode: 93, duration: 0.125s, episode steps: 24, steps per second: 192, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.104 [-0.962, 0.392], loss: 0.821628, mean_absolute_error: 5.300941, mean_q: 10.246982\n",
      " 1752/5000: episode: 94, duration: 0.107s, episode steps: 22, steps per second: 205, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.100 [-1.227, 0.396], loss: 1.407068, mean_absolute_error: 5.409943, mean_q: 10.297561\n",
      " 1799/5000: episode: 95, duration: 0.243s, episode steps: 47, steps per second: 193, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.018 [-0.957, 0.626], loss: 1.549266, mean_absolute_error: 5.493465, mean_q: 10.365453\n",
      " 1822/5000: episode: 96, duration: 0.112s, episode steps: 23, steps per second: 205, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.068 [-0.984, 0.454], loss: 1.547398, mean_absolute_error: 5.635954, mean_q: 10.658779\n",
      " 1834/5000: episode: 97, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.115 [-1.184, 0.584], loss: 1.090889, mean_absolute_error: 5.542601, mean_q: 10.664989\n",
      " 1856/5000: episode: 98, duration: 0.118s, episode steps: 22, steps per second: 187, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.055 [-1.485, 0.823], loss: 1.285383, mean_absolute_error: 5.689332, mean_q: 11.070740\n",
      " 1872/5000: episode: 99, duration: 0.089s, episode steps: 16, steps per second: 180, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.091 [-2.041, 1.173], loss: 1.275650, mean_absolute_error: 5.827371, mean_q: 11.329748\n",
      " 1884/5000: episode: 100, duration: 0.081s, episode steps: 12, steps per second: 149, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.145 [-2.120, 1.142], loss: 1.357428, mean_absolute_error: 5.823697, mean_q: 11.217667\n",
      " 1897/5000: episode: 101, duration: 0.086s, episode steps: 13, steps per second: 152, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.120 [-1.294, 0.609], loss: 2.125686, mean_absolute_error: 5.894821, mean_q: 11.191094\n",
      " 1912/5000: episode: 102, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.109 [-1.054, 0.580], loss: 1.128918, mean_absolute_error: 5.889416, mean_q: 11.317326\n",
      " 1927/5000: episode: 103, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.089 [-1.015, 0.417], loss: 1.816136, mean_absolute_error: 5.945694, mean_q: 11.301665\n",
      " 1953/5000: episode: 104, duration: 0.133s, episode steps: 26, steps per second: 195, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.067 [-1.098, 0.414], loss: 2.005362, mean_absolute_error: 6.059731, mean_q: 11.514562\n",
      " 1965/5000: episode: 105, duration: 0.063s, episode steps: 12, steps per second: 191, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.115 [-1.432, 0.774], loss: 2.462622, mean_absolute_error: 6.151554, mean_q: 11.686310\n",
      " 1978/5000: episode: 106, duration: 0.073s, episode steps: 13, steps per second: 177, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.114 [-1.394, 0.783], loss: 1.379837, mean_absolute_error: 6.091088, mean_q: 11.685851\n",
      " 1993/5000: episode: 107, duration: 0.076s, episode steps: 15, steps per second: 196, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.082 [-1.276, 0.618], loss: 2.155904, mean_absolute_error: 6.178441, mean_q: 11.825037\n",
      " 2005/5000: episode: 108, duration: 0.065s, episode steps: 12, steps per second: 185, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.100 [-2.600, 1.614], loss: 2.466819, mean_absolute_error: 6.284431, mean_q: 11.980779\n",
      " 2028/5000: episode: 109, duration: 0.112s, episode steps: 23, steps per second: 206, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.041 [-2.252, 1.409], loss: 1.674444, mean_absolute_error: 6.300728, mean_q: 12.018456\n",
      " 2041/5000: episode: 110, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.119 [-2.310, 1.377], loss: 2.268226, mean_absolute_error: 6.419584, mean_q: 12.212989\n",
      " 2056/5000: episode: 111, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.104 [-1.150, 0.586], loss: 2.464963, mean_absolute_error: 6.364563, mean_q: 11.982702\n",
      " 2068/5000: episode: 112, duration: 0.060s, episode steps: 12, steps per second: 201, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.104 [-1.267, 0.803], loss: 2.399910, mean_absolute_error: 6.458011, mean_q: 12.137031\n",
      " 2090/5000: episode: 113, duration: 0.131s, episode steps: 22, steps per second: 168, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.058 [-1.094, 0.438], loss: 2.003777, mean_absolute_error: 6.438937, mean_q: 12.184948\n",
      " 2125/5000: episode: 114, duration: 0.210s, episode steps: 35, steps per second: 167, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.056 [-1.017, 0.376], loss: 2.132437, mean_absolute_error: 6.500481, mean_q: 12.325526\n",
      " 2151/5000: episode: 115, duration: 0.139s, episode steps: 26, steps per second: 186, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.084 [-1.154, 0.380], loss: 2.131653, mean_absolute_error: 6.561519, mean_q: 12.506208\n",
      " 2176/5000: episode: 116, duration: 0.141s, episode steps: 25, steps per second: 178, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.056 [-0.989, 0.380], loss: 1.813898, mean_absolute_error: 6.631648, mean_q: 12.789333\n",
      " 2190/5000: episode: 117, duration: 0.069s, episode steps: 14, steps per second: 204, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.110 [-1.100, 0.600], loss: 2.674223, mean_absolute_error: 6.730167, mean_q: 12.782762\n",
      " 2205/5000: episode: 118, duration: 0.074s, episode steps: 15, steps per second: 202, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.107 [-1.228, 0.575], loss: 2.430698, mean_absolute_error: 6.780464, mean_q: 12.882341\n",
      " 2220/5000: episode: 119, duration: 0.080s, episode steps: 15, steps per second: 188, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.114 [-0.956, 0.415], loss: 2.732265, mean_absolute_error: 6.851901, mean_q: 12.921985\n",
      " 2242/5000: episode: 120, duration: 0.108s, episode steps: 22, steps per second: 204, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-0.960, 0.268], loss: 2.289646, mean_absolute_error: 6.840105, mean_q: 12.982641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2270/5000: episode: 121, duration: 0.143s, episode steps: 28, steps per second: 195, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.111 [-0.984, 0.188], loss: 3.526611, mean_absolute_error: 6.979375, mean_q: 13.039157\n",
      " 2309/5000: episode: 122, duration: 0.190s, episode steps: 39, steps per second: 205, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.023 [-1.046, 0.398], loss: 2.841224, mean_absolute_error: 6.982582, mean_q: 13.088970\n",
      " 2328/5000: episode: 123, duration: 0.125s, episode steps: 19, steps per second: 152, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.119 [-1.133, 0.375], loss: 2.596931, mean_absolute_error: 6.983477, mean_q: 13.252357\n",
      " 2380/5000: episode: 124, duration: 0.306s, episode steps: 52, steps per second: 170, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.093 [-0.462, 0.808], loss: 2.924142, mean_absolute_error: 7.163630, mean_q: 13.496029\n",
      " 2412/5000: episode: 125, duration: 0.152s, episode steps: 32, steps per second: 211, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.071 [-0.923, 0.219], loss: 3.418389, mean_absolute_error: 7.208558, mean_q: 13.446272\n",
      " 2434/5000: episode: 126, duration: 0.114s, episode steps: 22, steps per second: 192, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.102 [-0.938, 0.426], loss: 2.064955, mean_absolute_error: 7.225748, mean_q: 13.767642\n",
      " 2456/5000: episode: 127, duration: 0.107s, episode steps: 22, steps per second: 206, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.088 [-0.937, 0.257], loss: 2.298470, mean_absolute_error: 7.256308, mean_q: 13.897665\n",
      " 2536/5000: episode: 128, duration: 0.385s, episode steps: 80, steps per second: 208, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.016 [-0.892, 0.442], loss: 3.166169, mean_absolute_error: 7.385481, mean_q: 13.957434\n",
      " 2627/5000: episode: 129, duration: 0.512s, episode steps: 91, steps per second: 178, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.100 [-0.436, 0.918], loss: 2.990886, mean_absolute_error: 7.563529, mean_q: 14.340658\n",
      " 2685/5000: episode: 130, duration: 0.284s, episode steps: 58, steps per second: 204, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.026 [-0.943, 0.376], loss: 2.523311, mean_absolute_error: 7.636903, mean_q: 14.614156\n",
      " 2716/5000: episode: 131, duration: 0.148s, episode steps: 31, steps per second: 209, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.089 [-1.179, 0.398], loss: 2.798162, mean_absolute_error: 7.761565, mean_q: 14.828184\n",
      " 2764/5000: episode: 132, duration: 0.239s, episode steps: 48, steps per second: 201, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.022 [-0.915, 0.415], loss: 3.243893, mean_absolute_error: 7.831141, mean_q: 14.814964\n",
      " 2806/5000: episode: 133, duration: 0.244s, episode steps: 42, steps per second: 172, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.039 [-0.953, 0.422], loss: 2.776780, mean_absolute_error: 7.827852, mean_q: 14.841722\n",
      " 2834/5000: episode: 134, duration: 0.165s, episode steps: 28, steps per second: 169, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.082 [-0.942, 0.246], loss: 2.585966, mean_absolute_error: 7.905814, mean_q: 15.161398\n",
      " 2857/5000: episode: 135, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.100 [-1.006, 0.210], loss: 1.907411, mean_absolute_error: 7.880733, mean_q: 15.199687\n",
      " 2904/5000: episode: 136, duration: 0.231s, episode steps: 47, steps per second: 203, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: -0.056 [-1.000, 0.364], loss: 3.601627, mean_absolute_error: 8.110178, mean_q: 15.351858\n",
      " 2930/5000: episode: 137, duration: 0.126s, episode steps: 26, steps per second: 206, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.089 [-0.930, 0.226], loss: 4.072438, mean_absolute_error: 8.179632, mean_q: 15.445807\n",
      " 2959/5000: episode: 138, duration: 0.148s, episode steps: 29, steps per second: 196, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.078 [-1.027, 0.416], loss: 3.456939, mean_absolute_error: 8.220151, mean_q: 15.514253\n",
      " 3000/5000: episode: 139, duration: 0.209s, episode steps: 41, steps per second: 196, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.065 [-1.019, 0.352], loss: 3.290115, mean_absolute_error: 8.268749, mean_q: 15.746763\n",
      " 3021/5000: episode: 140, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.098 [-1.033, 0.413], loss: 2.954498, mean_absolute_error: 8.329944, mean_q: 15.839738\n",
      " 3075/5000: episode: 141, duration: 0.323s, episode steps: 54, steps per second: 167, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.044 [-0.414, 0.837], loss: 2.555620, mean_absolute_error: 8.332973, mean_q: 15.946836\n",
      " 3114/5000: episode: 142, duration: 0.188s, episode steps: 39, steps per second: 207, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.108 [-0.894, 0.250], loss: 2.767980, mean_absolute_error: 8.379541, mean_q: 16.056990\n",
      " 3147/5000: episode: 143, duration: 0.165s, episode steps: 33, steps per second: 200, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.140 [-0.208, 1.123], loss: 3.912368, mean_absolute_error: 8.528482, mean_q: 16.214525\n",
      " 3186/5000: episode: 144, duration: 0.196s, episode steps: 39, steps per second: 199, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.065 [-0.876, 0.200], loss: 3.411069, mean_absolute_error: 8.511518, mean_q: 16.193844\n",
      " 3251/5000: episode: 145, duration: 0.322s, episode steps: 65, steps per second: 202, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.071 [-0.795, 0.886], loss: 2.965816, mean_absolute_error: 8.639321, mean_q: 16.595474\n",
      " 3275/5000: episode: 146, duration: 0.144s, episode steps: 24, steps per second: 166, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.125 [-0.822, 0.193], loss: 3.280158, mean_absolute_error: 8.721778, mean_q: 16.735086\n",
      " 3306/5000: episode: 147, duration: 0.184s, episode steps: 31, steps per second: 169, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.113 [-1.167, 0.199], loss: 4.787257, mean_absolute_error: 8.741434, mean_q: 16.513401\n",
      " 3365/5000: episode: 148, duration: 0.294s, episode steps: 59, steps per second: 201, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.051 [-0.822, 0.201], loss: 3.238681, mean_absolute_error: 8.867242, mean_q: 16.977018\n",
      " 3425/5000: episode: 149, duration: 0.295s, episode steps: 60, steps per second: 203, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.089 [-0.751, 0.616], loss: 3.228627, mean_absolute_error: 8.911301, mean_q: 17.140873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3519/5000: episode: 150, duration: 0.516s, episode steps: 94, steps per second: 182, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.033 [-0.542, 0.981], loss: 3.364593, mean_absolute_error: 9.057568, mean_q: 17.336023\n",
      " 3555/5000: episode: 151, duration: 0.200s, episode steps: 36, steps per second: 180, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.084 [-0.209, 0.753], loss: 3.621181, mean_absolute_error: 9.116926, mean_q: 17.485611\n",
      " 3599/5000: episode: 152, duration: 0.224s, episode steps: 44, steps per second: 197, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.082 [-0.248, 0.819], loss: 3.580888, mean_absolute_error: 9.158651, mean_q: 17.585537\n",
      " 3651/5000: episode: 153, duration: 0.257s, episode steps: 52, steps per second: 203, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.087 [-0.204, 0.797], loss: 4.316366, mean_absolute_error: 9.255388, mean_q: 17.705885\n",
      " 3719/5000: episode: 154, duration: 0.337s, episode steps: 68, steps per second: 202, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.075 [-0.415, 0.826], loss: 3.562371, mean_absolute_error: 9.337730, mean_q: 17.815882\n",
      " 3779/5000: episode: 155, duration: 0.363s, episode steps: 60, steps per second: 165, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.097 [-0.238, 0.960], loss: 4.128057, mean_absolute_error: 9.446116, mean_q: 18.058086\n",
      " 3808/5000: episode: 156, duration: 0.138s, episode steps: 29, steps per second: 211, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.114 [-0.870, 0.370], loss: 3.731068, mean_absolute_error: 9.524197, mean_q: 18.295832\n",
      " 3850/5000: episode: 157, duration: 0.208s, episode steps: 42, steps per second: 202, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.084 [-0.509, 0.945], loss: 4.303007, mean_absolute_error: 9.491585, mean_q: 18.123489\n",
      " 3898/5000: episode: 158, duration: 0.236s, episode steps: 48, steps per second: 203, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.077 [-0.382, 0.786], loss: 3.686870, mean_absolute_error: 9.512244, mean_q: 18.230375\n",
      " 3929/5000: episode: 159, duration: 0.149s, episode steps: 31, steps per second: 208, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.119 [-0.170, 0.720], loss: 3.714843, mean_absolute_error: 9.598125, mean_q: 18.457577\n",
      " 3987/5000: episode: 160, duration: 0.347s, episode steps: 58, steps per second: 167, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.089 [-0.885, 0.317], loss: 4.002770, mean_absolute_error: 9.742290, mean_q: 18.754459\n",
      " 4029/5000: episode: 161, duration: 0.229s, episode steps: 42, steps per second: 183, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.120 [-0.248, 0.759], loss: 3.287796, mean_absolute_error: 9.721979, mean_q: 18.734917\n",
      " 4065/5000: episode: 162, duration: 0.174s, episode steps: 36, steps per second: 207, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.130 [-0.321, 0.858], loss: 3.412172, mean_absolute_error: 9.840384, mean_q: 19.017975\n",
      " 4106/5000: episode: 163, duration: 0.205s, episode steps: 41, steps per second: 200, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.101 [-0.786, 0.170], loss: 3.970820, mean_absolute_error: 9.982891, mean_q: 19.197294\n",
      " 4138/5000: episode: 164, duration: 0.155s, episode steps: 32, steps per second: 207, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.126 [-0.387, 1.044], loss: 4.896530, mean_absolute_error: 10.023252, mean_q: 19.131096\n",
      " 4178/5000: episode: 165, duration: 0.209s, episode steps: 40, steps per second: 192, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.116 [-0.764, 0.367], loss: 4.232239, mean_absolute_error: 10.039822, mean_q: 19.229446\n",
      " 4254/5000: episode: 166, duration: 0.435s, episode steps: 76, steps per second: 175, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.059 [-0.775, 0.338], loss: 4.757504, mean_absolute_error: 10.096265, mean_q: 19.341137\n",
      " 4314/5000: episode: 167, duration: 0.292s, episode steps: 60, steps per second: 205, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.082 [-0.503, 0.796], loss: 3.501386, mean_absolute_error: 10.120526, mean_q: 19.527613\n",
      " 4384/5000: episode: 168, duration: 0.348s, episode steps: 70, steps per second: 201, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.068 [-0.602, 0.724], loss: 3.809714, mean_absolute_error: 10.301141, mean_q: 19.886261\n",
      " 4435/5000: episode: 169, duration: 0.278s, episode steps: 51, steps per second: 183, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.106 [-1.000, 0.240], loss: 4.118975, mean_absolute_error: 10.412174, mean_q: 20.009844\n",
      " 4484/5000: episode: 170, duration: 0.292s, episode steps: 49, steps per second: 168, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.096 [-0.371, 0.716], loss: 4.390118, mean_absolute_error: 10.508512, mean_q: 20.207327\n",
      " 4572/5000: episode: 171, duration: 0.435s, episode steps: 88, steps per second: 202, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.078 [-0.468, 0.724], loss: 4.132530, mean_absolute_error: 10.506866, mean_q: 20.270313\n",
      " 4639/5000: episode: 172, duration: 0.329s, episode steps: 67, steps per second: 204, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.106 [-0.339, 0.805], loss: 4.210408, mean_absolute_error: 10.619378, mean_q: 20.559074\n",
      " 4686/5000: episode: 173, duration: 0.274s, episode steps: 47, steps per second: 171, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.131 [-0.787, 0.241], loss: 3.622219, mean_absolute_error: 10.709238, mean_q: 20.824327\n",
      " 4765/5000: episode: 174, duration: 0.425s, episode steps: 79, steps per second: 186, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.072 [-0.517, 0.689], loss: 3.412597, mean_absolute_error: 10.827484, mean_q: 21.086191\n",
      " 4850/5000: episode: 175, duration: 0.412s, episode steps: 85, steps per second: 206, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.069 [-0.771, 0.606], loss: 4.488196, mean_absolute_error: 10.964010, mean_q: 21.209171\n",
      " 4924/5000: episode: 176, duration: 0.414s, episode steps: 74, steps per second: 179, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.102 [-0.796, 0.510], loss: 3.995158, mean_absolute_error: 11.097921, mean_q: 21.533329\n",
      " 4994/5000: episode: 177, duration: 0.369s, episode steps: 70, steps per second: 190, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.087 [-0.372, 0.728], loss: 4.516839, mean_absolute_error: 11.207765, mean_q: 21.714098\n",
      "done, took 33.091 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 47.000, steps: 47\n",
      "Episode 2: reward: 48.000, steps: 48\n",
      "Episode 3: reward: 58.000, steps: 58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 61.000, steps: 61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xece43c8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=5000, visualize=True, verbose=2)\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
